services:
  nginx:
    # Security hardening overrides
    read_only: true
    user: "101:101" # non-root (adjust if base image UID/GID differ)
    cap_drop: ["ALL"]
    cap_add: ["NET_BIND_SERVICE"]
    # security_opt removed here to avoid duplicate merged entries; base prod file already sets
    #   - no-new-privileges:true
    # Add seccomp once profile is available:
    # security_opt:
    #   - no-new-privileges:true
    #   - seccomp:./security/seccomp-tight.json
    tmpfs:
      # Specify uid/gid/mode so non-root (101) can create temp cache dirs without chmod logic
      - /var/cache/nginx:rw,noexec,nosuid,nodev,size=64m,uid=101,gid=101,mode=0755
      - /var/run:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=0755
    ulimits:
      nofile: 65536
    deploy:
      resources:
        limits:
          cpus: "0.50"
          memory: 256M
        reservations:
          memory: 128M
    volumes:
      # Persist SSL certs (Letâ€™s Encrypt); kept read-only inside container via :ro in base file
      - letsencrypt:/etc/letsencrypt

  cloudflared:
    image: cloudflare/cloudflared:latest
    command: ["tunnel","run"]
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
      # Optional: force HTTP/2 if QUIC buffer warnings/noisy logs are undesirable (uncomment to enable)
      # - TUNNEL_TRANSPORT_PROTOCOL=http2
    read_only: true
    cap_drop: ["ALL"]
    cap_add: ["NET_BIND_SERVICE"]
    security_opt:
      - no-new-privileges:true
      # - seccomp:./security/seccomp-tight.json
    restart: unless-stopped
    depends_on:
      - nginx
    healthcheck:
      # Use internal DNS name to verify edge can reach origin
      test: ["CMD-SHELL", "wget -q -O /dev/null http://nginx/_up || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 20s

  backend:
    # Disable secret mount locally on Windows; use env fallback instead
    secrets: []
    env_file:
      - ./secrets/backend.env
    environment:
      OPENAI_BASE_URL: "http://host.docker.internal:11434/v1"
      ANALYTICS_DB: "1"
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_API_KEY_FILE: ""  # neutralize file-based loading locally
      MODEL: "gpt-oss:20b"
      OLLAMA_HOST: host.docker.internal
      OLLAMA_PORT: "11434"
      DEV_ALLOW_NO_LLM: "0"
      # LLM warmup / timeout tuning (see OPERATIONS.md Model Warming section)
      LLM_CONNECT_TIMEOUT: "10"
      LLM_READ_TIMEOUT: "45"
      LLM_INITIAL_RETRY: "1"
      LLM_WARM_WINDOW_S: "60"
      MASTER_KEK_B64: ${MASTER_KEK_B64}
      ENCRYPTION_MASTER_KEY_BASE64: ${ENCRYPTION_MASTER_KEY_BASE64:-${MASTER_KEK_B64}}
      ENCRYPTION_ENABLED: "1"           # flipped on for KMS mode
      CRYPTO_STRICT_STARTUP: "0"        # allow init after start
      KMS_PROVIDER: "gcp"
      GCP_KMS_KEY: "projects/ledgermind-03445-3l/locations/us-east1/keyRings/ledgermind/cryptoKeys/kek"
      GCP_KMS_AAD: "app=ledgermind,env=prod"
      GOOGLE_APPLICATION_CREDENTIALS: "/secrets/gcp-sa.json"  # mount provided below
      GRPC_VERBOSITY: "ERROR"
      GRPC_TRACE: ""
    read_only: true
    user: "10001:10001"
    cap_drop: ["ALL"]
    security_opt:
      - no-new-privileges:true
      # - seccomp:./security/seccomp-tight.json
    tmpfs:
      - /tmp:rw,noexec,nosuid,nodev,size=64m
    volumes:
      # Mount the actual service account JSON file (host path is a directory containing the file)
      - ./secrets/gcp-sa.json/ledgermind-backend-sa.json:/secrets/gcp-sa.json:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request,sys;hdr={'Host':'backend'};\nfor u in ('http://127.0.0.1:8000/live','http://127.0.0.1:8000/healthz'):\n  try:\n    req=urllib.request.Request(u,headers=hdr);\n    with urllib.request.urlopen(req,timeout=2) as r:\n      if r.getcode()==200: sys.exit(0)\n  except Exception: pass\nsys.exit(1)"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 60s
    depends_on: []
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          memory: 256M

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "/bin/ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 45s

  ollama-seed:
    image: ollama/ollama:latest
    profiles: ["seed"]
    volumes:
      - ollama-data:/root/.ollama
    entrypoint: ["/bin/sh","-lc"]
    command: |
      set -e
      ollama pull llama3.1:8b    || true
      ollama pull phi3:3.8b      || true
      echo "Seeded models."
    restart: "no"

volumes:
  letsencrypt:
  ollama-data:

# Disable secrets root in local override to avoid Docker Desktop bind errors on Windows
secrets: {}
