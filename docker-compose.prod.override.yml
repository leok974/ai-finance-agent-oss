services:
  nginx:
    # Security hardening overrides
    read_only: true
    user: "101:101" # non-root (adjust if base image UID/GID differ)
    cap_drop: ["ALL"]
    cap_add: ["NET_BIND_SERVICE"]
    # security_opt removed here to avoid duplicate merged entries; base prod file already sets
    #   - no-new-privileges:true
    # Add seccomp once profile is available:
    # security_opt:
    #   - no-new-privileges:true
    #   - seccomp:./security/seccomp-tight.json
    tmpfs:
      # Specify uid/gid/mode so non-root (101) can create temp cache dirs without chmod logic
      - /var/cache/nginx:rw,noexec,nosuid,nodev,size=64m,uid=101,gid=101,mode=0755
      - /var/run:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=0755
    ulimits:
      nofile: 65536
    deploy:
      resources:
        limits:
          cpus: "0.50"
          memory: 256M
        reservations:
          memory: 128M
    volumes:
      # Persist SSL certs (Letâ€™s Encrypt); kept read-only inside container via :ro in base file
      - letsencrypt:/etc/letsencrypt

  cloudflared:
    image: cloudflare/cloudflared:latest
    # Credentials-file mode (authoritative). Place <UUID>.json + config.yml under ./cloudflared
    # Acquire credentials:
    #   cloudflared tunnel login
    #   cloudflared tunnel create ledgermind-prod   (produces <UUID>.json)
    # Create ./cloudflared/config.yml with:
    #   tunnel: <UUID>
    #   credentials-file: /etc/cloudflared/<UUID>.json
    #   ingress:
    #     - hostname: app.ledger-mind.org
    #       service: http://nginx:80
    #     - service: http_status:404
    command: [
      "tunnel",
      "--config","/etc/cloudflared/config.yml",
      "--no-autoupdate",
      "--loglevel","info",
      "--protocol","auto",
      "--metrics","0.0.0.0:2000",
      "run"
    ]
    environment:
      # Ensure token mode is fully disabled even if base compose supplies it
      TUNNEL_TOKEN: ""
    volumes:
      - ./cloudflared:/etc/cloudflared:ro
    read_only: true
    cap_drop: ["ALL"]
    cap_add: ["NET_BIND_SERVICE"]
    security_opt:
      - no-new-privileges:true
      # - seccomp:./security/seccomp-tight.json
    restart: unless-stopped
    depends_on:
      - nginx
    healthcheck:
      # Simple metrics probe; exits non-zero if metric missing
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:2000/metrics | grep -q cloudflared_tunnel_ha_connections"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s

  backend:
    # Disable secret mount locally on Windows; use env fallback instead
    secrets: []
    env_file:
      - ./secrets/backend.env
    environment:
      OPENAI_BASE_URL: "http://host.docker.internal:11434/v1"
      ANALYTICS_DB: "1"
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      OPENAI_API_KEY_FILE: ""  # neutralize file-based loading locally
      MODEL: "gpt-oss:20b"
      OLLAMA_HOST: host.docker.internal
      OLLAMA_PORT: "11434"
      DEV_ALLOW_NO_LLM: "0"
      # LLM warmup / timeout tuning (see OPERATIONS.md Model Warming section)
      LLM_CONNECT_TIMEOUT: "10"
      LLM_READ_TIMEOUT: "45"
      LLM_INITIAL_RETRY: "1"
      LLM_WARM_WINDOW_S: "60"
      MASTER_KEK_B64: ${MASTER_KEK_B64}
      ENCRYPTION_MASTER_KEY_BASE64: ${ENCRYPTION_MASTER_KEY_BASE64:-${MASTER_KEK_B64}}
      ENCRYPTION_ENABLED: "1"           # flipped on for KMS mode
      CRYPTO_STRICT_STARTUP: "0"        # allow init after start
      KMS_PROVIDER: "gcp"
      GCP_KMS_KEY: "projects/ledgermind-03445-3l/locations/us-east1/keyRings/ledgermind/cryptoKeys/kek"
      GCP_KMS_AAD: "app=ledgermind,env=prod"
      GOOGLE_APPLICATION_CREDENTIALS: "/secrets/gcp-sa.json"  # mount provided below
      GRPC_VERBOSITY: "ERROR"
      GRPC_TRACE: ""
      # Unified Help API caching / rephrase knobs
      HELP_TTL_SECONDS: "86400"
      REPHRASE_VERSION: "v1"
      PRIMARY_MODEL_TAG: "gpt-oss:20b"
      # Build metadata (injected at deploy time by CI or local script)
      BACKEND_BRANCH: ${BACKEND_BRANCH:-unknown}
      BACKEND_COMMIT: ${BACKEND_COMMIT:-unknown}
    read_only: true
    user: "10001:10001"
    cap_drop: ["ALL"]
    security_opt:
      - no-new-privileges:true
      # - seccomp:./security/seccomp-tight.json
    tmpfs:
      - /tmp:rw,noexec,nosuid,nodev,size=64m
    volumes:
      # Mount the actual service account JSON file (host path is a directory containing the file)
      - ./secrets/gcp-sa.json/ledgermind-backend-sa.json:/secrets/gcp-sa.json:ro
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request,sys;hdr={'Host':'backend'};\nfor u in ('http://127.0.0.1:8000/live','http://127.0.0.1:8000/healthz'):\n  try:\n    req=urllib.request.Request(u,headers=hdr);\n    with urllib.request.urlopen(req,timeout=2) as r:\n      if r.getcode()==200: sys.exit(0)\n  except Exception: pass\nsys.exit(1)"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 60s
    depends_on: []
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 512M
        reservations:
          memory: 256M

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_MODELS=/root/.ollama
    volumes:
      - ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "/bin/ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 30
      start_period: 45s

  ollama-seed:
    image: ollama/ollama:latest
    profiles: ["seed"]
    volumes:
      - ollama-data:/root/.ollama
    entrypoint: ["/bin/sh","-lc"]
    command: |
      set -e
      ollama pull llama3.1:8b    || true
      ollama pull phi3:3.8b      || true
      echo "Seeded models."
    restart: "no"

volumes:
  letsencrypt:
  ollama-data:

# Disable secrets root in local override to avoid Docker Desktop bind errors on Windows
secrets: {}
