# LLM Health Check - Quick Reference

## ‚úÖ What Was Fixed

**Problem:** `/agent/status` hardcoded to probe `localhost:11434` ‚Üí always failed in Docker
**Solution:** Created `llm_health.ping_llm()` using correct service name (`http://ollama:11434`)
**Result:** Health check now accurately reports LLM availability ‚úÖ

---

## üîç Quick Tests

### 1. Health Check
```bash
curl -sL http://localhost/agent/status | jq '.llm_ok'
# Expected: true
```

### 2. Model List
```bash
curl -sL http://localhost/agent/models | jq '.models[]? | .id'
# Expected: ["gpt-oss:20b", "default", "nomic-embed-text:latest"]
```

### 3. Warmup
```bash
curl -sL -X POST http://localhost/api/agent/warmup | jq '.ok, .took_ms'
# Expected: true, <10
```

### 4. Chat
```bash
curl -sL http://localhost/api/agent/gpt \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Say hi"}]}' \
  | jq -r '.model'
# Expected: gpt-oss:20b (not "deterministic")
```

---

## üìÅ Files Modified

| File | Purpose | Lines |
|------|---------|-------|
| `apps/backend/app/services/llm_health.py` | New health check service | 83 |
| `apps/backend/app/routers/agent.py` | Updated `/agent/status` endpoint | 1577-1600 |

---

## üîß How It Works

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ /agent/status   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ llm_health.ping_llm ‚îÇ
‚îÇ ‚Ä¢ Uses OLLAMA_BASE_URL
‚îÇ ‚Ä¢ 5s TTL cache
‚îÇ ‚Ä¢ Provider-aware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ http://ollama:11434  ‚îÇ
‚îÇ GET /api/tags        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚öôÔ∏è Configuration

### Required Env Vars (Already Set)
```bash
OLLAMA_BASE_URL=http://ollama:11434
OPENAI_BASE_URL=http://ollama:11434/v1
DEFAULT_LLM_PROVIDER=ollama
```

### Docker Service Name
```yaml
# docker-compose.prod.yml
services:
  ollama:
    image: ollama/ollama:latest
    # Backend resolves this as "http://ollama:11434"
```

---

## üêõ Troubleshooting

### Health check fails?
```bash
# 1. Check DNS
docker exec backend getent hosts ollama

# 2. Test connectivity
docker exec backend curl -sf http://ollama:11434/api/tags

# 3. Check env vars
docker exec backend printenv | grep OLLAMA_BASE_URL
```

### Clear cache
```python
from app.services.llm_health import clear_health_cache
clear_health_cache()
```

---

## üìä Expected Response

### Success
```json
{
  "ok": true,
  "llm_ok": true,
  "provider": "ollama",
  "base_url": "http://ollama:11434",
  "model": "gpt-oss:20b"
}
```

### Failure
```json
{
  "ok": false,
  "llm_ok": false,
  "error": "Connection refused: ...",
  "provider": "ollama",
  "base_url": "http://ollama:11434"
}
```

---

## üéØ Why This Matters

Before:
- ‚ùå Health check: "Connection refused"
- ‚úÖ Chat: Works fine (LLM actually available)
- ü§î UI: Disables features based on broken health signal

After:
- ‚úÖ Health check: Accurate status
- ‚úÖ Chat: Works fine
- ‚úÖ UI: Can enable features when LLM available

---

## üìö Related Docs

- Full details: `docs/llm-health-fix-summary.md`
- Dev unlock: `docs/LEDGERMIND_DEV_OVERRIDE.md`
- Validation results: `docs/dev-endpoints-validation.md`
