services:
  ollama:
    environment:
      OLLAMA_KEEP_ALIVE: 5m
    networks:
      - shared-ollama

  backend:
    build:
      context: .
      dockerfile: deploy/Dockerfile.backend.slim
    command:
      - python
      - -m
      - uvicorn
      - app.main:app
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --proxy-headers
      - --timeout-keep-alive
      - "65"

  nginx:
    read_only: true
    tmpfs:
      - /tmp:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=1777
      # existing tmpfs mounts for cache/run already defined in base prod file

networks:
  shared-ollama:
    external: true
