# Database
# SQLite (default for quick start)
DATABASE_URL=sqlite:///./data/finance.db

# PostgreSQL (preferred in dev; matches docker-compose.yml)
# Start DB: docker compose up -d postgres
# Then use:
# DATABASE_URL=postgresql+psycopg://myuser:password@localhost:5432/finance

# OAuth providers (optional)
OAUTH_GITHUB_CLIENT_ID=
OAUTH_GITHUB_CLIENT_SECRET=
OAUTH_GOOGLE_CLIENT_ID=
OAUTH_GOOGLE_CLIENT_SECRET=

# After login, redirect web app URL
OAUTH_POST_LOGIN_REDIRECT=http://localhost:5173/app

# Dev-only bypass (0 by default; set to 1 only for quick spikes)
DEV_ALLOW_NO_AUTH=0
DEV_ALLOW_NO_CSRF=0
DEV_ALLOW_NO_LLM=0

# Environment
# prod | dev | test (prefer setting APP_ENV; ENV is legacy)
APP_ENV=dev
# ENV=dev

# LLM (OpenAI-compatible)
# For Ollama local server, keep defaults; for OpenAI/vLLM, change appropriately
OPENAI_BASE_URL=http://localhost:11434/v1
OPENAI_API_KEY=ollama
# Default model (Ollama tag or OpenAI model id)
MODEL=gpt-oss:20b

# --- NVIDIA NIM Configuration (Hackathon) ---
# NIM LLM Service (for chat completions)
NIM_LLM_URL=http://nim-llm-service:8000/v1
# For local testing with NVIDIA hosted API:
# NIM_LLM_URL=https://integrate.api.nvidia.com/v1
NIM_LLM_MODEL=meta/llama-3.1-nemotron-nano-8b-v1
NIM_API_KEY=  # Get from https://org.ngc.nvidia.com/setup/api-key

# NIM Embedding Service (for RAG)
NIM_EMBED_URL=http://nim-embed-service:8001/v1
# For local testing with NVIDIA hosted API:
# NIM_EMBED_URL=https://integrate.api.nvidia.com/v1
NIM_EMBED_MODEL=nvidia/nv-embed-v2
EMBED_DIM=768  # nv-embed-v2 output dimension

# Provider Selection
# DEFAULT_LLM_PROVIDER: "openai" (Ollama/OpenAI) or "nim" (NVIDIA NIM)
DEFAULT_LLM_PROVIDER=openai  # Change to "nim" for hackathon deployment
# EMBED_PROVIDER: "ollama", "openai", or "nim"
EMBED_PROVIDER=ollama  # Change to "nim" for hackathon deployment

# Cookies (dev)
COOKIE_SAMESITE=lax
COOKIE_SECURE=0

# Production switches
#COOKIE_SECURE=1
#COOKIE_SAMESITE=lax   # or "none" if truly cross-site under HTTPS
#COOKIE_DOMAIN=your.app.domain
#OAUTH_POST_LOGIN_REDIRECT=https://your.app.domain/app


# --- Encryption (Envelope AES-GCM) ---
# Master Key (KEK) in dev: either a 32-byte base64 string or hex; for production, use KMS/HSM.
# If not provided, the app will generate a random KEK on first boot and write a wrapped DEK only (not persisted KEK).
ENCRYPTION_ENABLED=1
ENCRYPTION_MASTER_KEY_BASE64=
# Active DEK label alias (kept flexible for future rotation)
ENCRYPTION_ACTIVE_LABEL=active

# --- ML Suggestion Controls (Canary + Calibration) ---
# Shadow mode: always predict with model (for comparison metrics) but use rules for actual suggestions
SUGGEST_ENABLE_SHADOW=1

# Canary rollout percentage: "0" (rules only), "10%" (10% users), "50%", "100" (all users)
# Uses sticky hash based on user_id for consistent A/B assignment
SUGGEST_USE_MODEL_CANARY=0

# Per-class confidence thresholds (JSON format)
# Model predictions below threshold fall back to rules
SUGGEST_THRESHOLDS_JSON={"Groceries":0.70,"Dining":0.75,"Shopping":0.65,"Transport":0.65,"Subscriptions":0.70,"Entertainment":0.60}

# Isotonic calibration: improves probability estimates using validation set
ML_CALIBRATION_ENABLED=1

# Auto-deployment thresholds for training
# Macro F1 threshold (all classes average)
ML_DEPLOY_THRESHOLD_F1=0.72
# Minimum per-class F1 (acceptance gate: all classes must meet this)
ML_DEPLOY_THRESHOLD_F1_MIN=0.60

