services:
  ollama:
    environment:
      OLLAMA_KEEP_ALIVE: 5m
      OMP_NUM_THREADS: "8"
    networks:
      - shared-ollama

  backend:
    build:
      context: .
      dockerfile: deploy/Dockerfile.backend.slim
    command:
      - python
      - -m
      - uvicorn
      - app.main:app
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --workers
      - "1"
      - --loop
      - uvloop
      - --http
      - h11
      - --proxy-headers
      - --timeout-keep-alive
      - "5"
      - --limit-concurrency
      - "8"

  ollama-warmup:
    image: curlimages/curl:8.7.1
    depends_on:
      ollama:
        condition: service_started
      backend:
        condition: service_started
    command:
      - sh
      - -c
      - >-
        set -euo pipefail;
        echo "[warmup] waiting for backend warmup endpoint";
        attempt=1;
        while [ $attempt -le 20 ]; do
          if curl -sf http://backend:8000/agent/warmup > /dev/null; then
            echo "[warmup] backend warmup succeeded";
            exit 0;
          fi;
          echo "[warmup] attempt ${attempt} failed; retrying";
          attempt=$((attempt + 1));
          sleep 3;
        done;
        echo "[warmup] backend warmup failed" >&2;
        exit 1
    restart: "no"

  nginx:
    read_only: true
    tmpfs:
      - /tmp:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=1777
      # existing tmpfs mounts for cache/run already defined in base prod file

networks:
  shared-ollama:
    external: true
