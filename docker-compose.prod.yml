# =============================================================================
# AI / COPILOT GUARDRAIL â€” DOCKER-COMPOSE SERVICE CONTRACTS
#
# These names and networks are part of the production routing contract.
# Cloudflared and nginx config expect them to stay stable.
#
# IF YOU ARE AN AI ASSISTANT (COPILOT / CHATGPT / ETC), FOLLOW THESE RULES:
#
# 1) DO NOT RENAME THESE SERVICES:
#      - nginx
#      - cloudflared
#      - backend (if referenced by nginx)
#    - The name "nginx" is used in cloudflared/config.yml as: service: http://nginx:80
#
# 2) DO NOT REMOVE nginx OR cloudflared FROM THEIR SHARED NETWORK.
#    - The tunnel must be able to resolve "nginx" via Docker DNS.
#    - If you change networks, you MUST update cloudflared/config.yml accordingly.
#
# 3) WHEN CHANGING PORTS OR NETWORKS:
#    - Update: deploy/nginx.conf, cloudflared/config.yml, and E2E tests together.
#    - Then run the prod verification commands (see nginx.conf header).
#
# 4) NEVER EXPOSE INTERNAL BACKEND PORTS DIRECTLY TO THE INTERNET BY ACCIDENT.
#    - Keep backend reachable only via nginx unless you know exactly why you're
#      exposing it and have updated security docs accordingly.
# =============================================================================

services:
  postgres:
    container_name: lm-postgres
    image: pgvector/pgvector:pg15
    restart: unless-stopped
    environment:
      POSTGRES_USER: lm
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
      POSTGRES_DB: lm
    volumes:
      - lm-pgdata:/var/lib/postgresql/data
    secrets:
      - db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U lm -d lm || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 20
    # No host port - avoid conflicts
    networks:
      - infra_net
      - shared-ollama

  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "60", "1000", "--loglevel", "warning"]
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 10
    ports:
      - "127.0.0.1:6379:6379"
    restart: unless-stopped
    networks:
      - shared-ollama

  backend:
    container_name: ai-finance-backend
    image: ledgermind-backend:main-no-ridehailing
    pull_policy: never
    restart: unless-stopped
    env_file:
      - ./secrets/backend.env
    environment:
      UVICORN_WORKERS: "${UVICORN_WORKERS:-2}"  # âœ… Updated default from 1 to 2
      WEB_CONCURRENCY: "${WEB_CONCURRENCY:-2}"
      LOG_LEVEL: "info"
      ENV: "prod"
      DEBUG: "0"
      APP_ENV: "prod"
      ENCRYPTION_ENABLED: "1"
      CRYPTO_STRICT_STARTUP: ${CRYPTO_STRICT_STARTUP:-0}
      GCP_KMS_KEY: "projects/ledgermind-03445-3l/locations/us-east1/keyRings/ledgermind/cryptoKeys/kek"
      GCP_KMS_AAD: "app=ledgermind,env=prod"
      GOOGLE_APPLICATION_CREDENTIALS: /secrets/gcp-sa.json
      # Set CRYPTO_REQUIRED=false to allow startup without functioning KMS (downgrades errors to warnings)
      CRYPTO_REQUIRED: "false"
      # Use DATABASE_URL_FILE secret indirection (populated via entrypoint sh wrapper)
      DATABASE_URL_FILE: /run/secrets/backend_db_url
      # Database connection robustness
      SQLA_POOL_PRE_PING: "1"
      DB_CONNECT_MAX_RETRIES: "30"
      DB_CONNECT_RETRY_SECONDS: "2"
      # Google OAuth
      ENABLE_GOOGLE_OAUTH: "1"
      CORS_ALLOW_ORIGINS: "https://ledger-mind.org,https://www.ledger-mind.org"
      FRONTEND_ORIGIN: "https://ledger-mind.org"
      COOKIE_SAMESITE: ${COOKIE_SAMESITE:-none}
      COOKIE_SECURE: ${COOKIE_SECURE:-1}
      COOKIE_DOMAIN: ${COOKIE_DOMAIN:-app.ledger-mind.org}
      TRUSTED_PROXY_CIDRS: "172.21.0.0/16"
      OPENAI_BASE_URL: "${OPENAI_BASE_URL:-http://ollama:11434/v1}"
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      OPENAI_FALLBACK_MODEL: "${OPENAI_FALLBACK_MODEL:-gpt-4o-mini}"
      MODEL: "${MODEL:-gpt-oss:20b}"
      # E2E testing disabled in production - use staging for test modes
      DEV_ALLOW_NO_LLM: "${DEV_ALLOW_NO_LLM:-0}"
      # Database pool sizing (prevent pool starvation under load)
      DB_POOL_SIZE: "${DB_POOL_SIZE:-10}"
      DB_MAX_OVERFLOW: "${DB_MAX_OVERFLOW:-20}"
      DB_POOL_PRE_PING: "${DB_POOL_PRE_PING:-1}"
      ALLOWED_HOSTS: "localhost,127.0.0.1,ledger-mind.org,app.ledger-mind.org,api.ledger-mind.org,nginx"
      ANALYTICS_DB: "1"
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LLM_CONNECT_TIMEOUT: "10"
      LLM_READ_TIMEOUT: "45"
      LLM_INITIAL_RETRY: "1"
      LLM_WARM_WINDOW_S: "60"
      KMS_PROVIDER: "gcp"
      GRPC_VERBOSITY: "ERROR"
      GRPC_TRACE: ""
      HELP_TTL_SECONDS: "86400"
      REPHRASE_VERSION: "v1"
      PRIMARY_MODEL_TAG: "gpt-oss:20b"
      BACKEND_BRANCH: ${BACKEND_BRANCH:-unknown}
      BACKEND_COMMIT: ${BACKEND_COMMIT:-unknown}
      SUGGESTIONS_ENABLED: "1"
      ML_FEEDBACK_SCORES_ENABLED: "${ML_FEEDBACK_SCORES_ENABLED:-1}"
      # Help/RAG
      HELP_USE_RAG: "${HELP_USE_RAG:-1}"
      RAG_TOP_K: "${RAG_TOP_K:-6}"
      RAG_MIN_SCORE: "${RAG_MIN_SCORE:-0.15}"
      HELP_CACHE_TTL_SEC: "${HELP_CACHE_TTL_SEC:-600}"
      REDIS_URL: "redis://redis:6379/0"
      # DevDiag
      DEVDIAG_TIMEOUT_S: "${DEVDIAG_TIMEOUT_S:-120}"
      DEVDIAG_ALLOW_HOSTS: "${DEVDIAG_ALLOW_HOSTS:-app.ledger-mind.org}"
    entrypoint: []
    command: >
      python -m uvicorn app.main:app
      --host 0.0.0.0
      --port 8000
      --workers ${UVICORN_WORKERS:-2}
      --limit-concurrency 200
      --timeout-keep-alive 20
      --proxy-headers
      --forwarded-allow-ips="*"
      --log-level ${LOG_LEVEL:-info}
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    secrets:
      - backend_db_url
    healthcheck:
      test:
        - "CMD"
        - "python"
        - "-c"
        - "import sys,urllib.request; sys.exit(0 if urllib.request.urlopen('http://127.0.0.1:8000/ready').getcode()==200 else 1)"
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    volumes:
      # Mount the actual service account JSON file (not the directory) so GOOGLE_APPLICATION_CREDENTIALS points to a file
      - ./secrets/gcp-sa.json/ledgermind-backend-sa.json:/secrets/gcp-sa.json:ro
    networks:
      infra_net:
        aliases:
          - backend
          - ai-finance-api.int
          - ledgermind-api.int
      shared-ollama:
        aliases:
          - backend
          - ai-finance-api.int
          - ledgermind-api.int
      applylens_applylens-prod:
        aliases:
          - ai-finance-api.int
          - ledgermind-api.int

  nginx:
    # AI/COPILOT: Service name MUST remain "nginx".
    # cloudflared uses service: http://nginx:80 in its config.
    # Changing this name will break the tunnel until configs + E2E are updated.
    build:
      context: .
      dockerfile: deploy/Dockerfile.nginx
      args:
        EDGE_GIT_COMMIT: ${GIT_COMMIT:-unknown}
        EDGE_BUILD_TIME: ${BUILD_TIME:-unknown}
        WEB_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        WEB_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        WEB_BUILD_ID: ${WEB_BUILD_ID:-not_set}
        VITE_SUGGESTIONS_ENABLED: ${VITE_SUGGESTIONS_ENABLED:-1}
        VITE_ANALYTICS_ENABLED: ${VITE_ANALYTICS_ENABLED:-0}
        # Empty string to use root paths (nginx proxies /api/* separately)
        VITE_API_BASE: ""
        VITE_GIT_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        VITE_GIT_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        GIT_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        GIT_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        BUILD_ID: ${BUILD_ID:-not_set}
        # OAuth provider toggles
        VITE_ENABLE_LOCAL_AUTH: "0"
        VITE_ENABLE_GOOGLE_OAUTH: "1"
        VITE_ENABLE_GITHUB_OAUTH: "0"
        # Standardized build metadata for buildStamp.ts
        VITE_BUILD_BRANCH: ${VITE_BUILD_BRANCH:-local}
        VITE_BUILD_COMMIT: ${VITE_BUILD_COMMIT:-dev}
        VITE_BUILD_TIME: ${VITE_BUILD_TIME:-unknown}
    image: ledgermind-web:main-no-ridehailing
    depends_on:
      backend:
        condition: service_healthy
      agui:
        condition: service_healthy
    volumes:
      - ./deploy/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./deploy/docker-entrypoint.d:/docker-entrypoint.d:ro
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt:ro
    # Ports disabled for tunnel-based setup - Cloudflare tunnel connects directly to nginx:80 internally
    ports:
      - "127.0.0.1:8083:80"  # Localhost-only for E2E testing bypass
    # Uncomment below for full local HTTPS:
    #   - "127.0.0.1:80:80"
    #   - "127.0.0.1:443:443"
    user: "101:101"
    read_only: true
    tmpfs:
      - /var/run:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=0755
      - /var/cache/nginx:rw,noexec,nosuid,nodev,size=64m,uid=101,gid=101,mode=0755
      - /tmp:rw,noexec,nosuid,nodev,size=32m,uid=101,gid=101,mode=1777
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # healthcheck:
    #   test: ["CMD-SHELL","wget -q -O /dev/null http://127.0.0.1:80/_up || exit 1"]
    #   interval: 10s
    #   timeout: 3s
    #   retries: 10
    #   start_period: 20s
    restart: unless-stopped
    networks:
      infra_net:
        aliases:
          - nginx
          - ai-finance.int
          - ledgermind-web.int
      shared-ollama:
        aliases:
          - nginx
          - ai-finance.int
          - ledgermind-web.int
      applylens_applylens-prod:
        aliases:
          - ai-finance.int
          - ledgermind-web.int


  certbot:
    image: certbot/certbot:latest
    volumes:
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt
    entrypoint: ["sh", "-c", "pip install --no-cache-dir certbot-dns-cloudflare && while :; do certbot renew --dns-cloudflare --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini --dns-cloudflare-propagation-seconds 120 --quiet || true; sleep 12h; done"]
    restart: unless-stopped

  nginx-reloader:
    image: docker:cli
    depends_on:
      - nginx
    environment:
      NGINX_CONTAINER: "${NGINX_CONTAINER:-ai-finance-agent-oss-clean-nginx-1}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      sh -c 'while :; do sleep 12h; docker exec "$NGINX_CONTAINER" nginx -s reload || true; done'
    restart: unless-stopped

  # cloudflared:
  #   # AI/COPILOT: Service name MUST remain "cloudflared".
  #   # Health monitoring and configs reference this name.
  #   # âš ï¸ DISABLED: Redundant local cloudflared causing race condition with remote CF tunnel.
  #   # The remote tunnel already routes to ledgermind-web.int and ledgermind-api.int.
  #   # See: LedgerMind tunnel stabilization (2025-01)
  #   image: cloudflare/cloudflared:latest
  #   command: tunnel run
  #   # Using credentials-file mode via mounted config/credentials. Remove token env to avoid ambiguity.
  #   # environment:
  #   #   TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN:-}
  #   volumes:
  #     - ./cloudflared:/etc/cloudflared:ro
  #   ports:
  #     - "2000:2000" # expose metrics for strict verification
  #   depends_on:
  #     nginx:
  #       condition: service_healthy
  #   # Note: healthcheck disabled - cloudflared image lacks shell/wget/curl
  #   # Tunnel status visible via metrics at localhost:2000/metrics (cloudflared_tunnel_ha_connections)
  #   restart: unless-stopped
  #   networks:
  #     - infra_net
  #     - shared-ollama

  # cf-health:
  #   # âš ï¸ DISABLED: Depends on local cloudflared which has been disabled.
  #   # See: LedgerMind tunnel stabilization (2025-01)
  #   image: curlimages/curl:8.10.1
  #   command: sh -lc "while :; do curl -sf http://cloudflared:2000/metrics | grep -q cloudflared_tunnel_ha_connections || exit 1; sleep 30; done"
  #   depends_on:
  #     cloudflared:
  #       condition: service_started
  #   restart: unless-stopped
  #   networks:
  #     - shared-ollama

  # Local models runtime (pull & serve models via Ollama API)
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    # Docker Desktop reliable GPU passthrough
    gpus: all
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    volumes:
      # Persist large model downloads across container recreations
      - ollama-models:/root/.ollama
    networks:
      - shared-ollama

  # Prometheus Pushgateway for metrics collection
  pushgateway:
    image: prom/pushgateway:latest
    restart: unless-stopped
    ports:
      - "9091:9091"
    networks:
      - shared-ollama
    command:
      - '--web.enable-admin-api'
      - '--push.disable-consistency-check'
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9091/-/healthy"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Database freshness exporter (run manually or via cron)
  freshness-exporter:
    image: python:3.11-slim
    restart: "no"
    working_dir: /work
    command: >
      bash -lc "pip install -q -r requirements.txt &&
                python db_freshness_push.py"
    environment:
      PGHOST: postgres
      PGPORT: 5432
      PGUSER: myuser
      PGPASSWORD_FILE: /run/secrets/db_password
      PGDATABASE: finance
      PGSCHEMA: public
      FRESHNESS_TABLES: transactions,transaction_labels,ml_features
      FRESHNESS_TIMESTAMP_COL: updated_at
      PUSHGATEWAY_URL: http://pushgateway:9091
      PUSH_JOB_NAME: dbt_source_freshness
      PUSH_INSTANCE: prod
    volumes:
      - ./ops/exporters:/work
    secrets:
      - db_password
    networks:
      - shared-ollama
    depends_on:
      postgres:
        condition: service_healthy
      pushgateway:
        condition: service_healthy

  # Sidecar to present 127.0.0.1:11434 inside backend netns, forwarding to ollama service

  agui:
    build:
      context: ./elysia-agui-gateway
    environment:
      BACKEND_BASE: http://backend:8000
      PORT: 3030
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "bun -e 'fetch(`http://127.0.0.1:3030/agui/ping`).then(r=>{if(r.ok)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))'"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 20s
    networks:
      - shared-ollama

# -----------------------------------------------------------------------------------
# LEDGERMIND / SHARED TUNNEL GUARDRAILS  (read this, Copilot ðŸ¤–)
#
# - This stack is behind a SINGLE shared Cloudflare Named Tunnel (ApplyLens tunnel).
# - The tunnel connectors are on `infra_net` network.
# - All public hostnames for LedgerMind are configured in the ApplyLens cloudflared config:
#     app.ledger-mind.org  -> http://ai-finance.int:80  (nginx)
#     api.ledger-mind.org  -> http://ai-finance-api.int:8000 (backend)
#
# RULES:
# - DO NOT remove `infra_net` from nginx or backend.
# - DO NOT rename or remove the aliases: `ai-finance.int`, `ai-finance-api.int`.
# - DO NOT introduce per-project cloudflared containers for LedgerMind.
# - DO NOT add any `httpHostHeader` overrides to cloudflared config for this stack.
# - Origin TLS is TERMINATED AT CLOUDFLARE. Inside Docker everything is plain HTTP.
#
# If you change service names, you MUST:
#   1) Update aliases here.
#   2) Update ApplyLens cloudflared config to match.
#   3) Run the health script in scripts/lm-health.ps1.
# -----------------------------------------------------------------------------------
networks:
  infra_net:
    external: true
  shared-ollama:
    external: true
  applylens_applylens-prod:
    external: true

volumes:
  lm-pgdata:
  redis_data:
  certbot_www:
  letsencrypt:
  # removed named volume 'ollama' in favor of bind mount for model store stability
  ollama-models:

secrets:
  openai_api_key:
    # Use absolute Windows path to avoid Docker Desktop bind issues on Windows
    file: C:/ai-finance-agent-oss-clean/secrets/openai_api_key
  db_password:
    file: ./secrets/db_password.txt
  backend_db_url:
    file: ./secrets/backend_database_url.txt
