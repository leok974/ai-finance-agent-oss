services:
  postgres:
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD_FILE: /run/secrets/db_password
      POSTGRES_DB: finance
    volumes:
      - pgdata:/var/lib/postgresql/data
    secrets:
      - db_password
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U myuser -d finance || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped
    networks:
      shared-ollama:
        aliases:
          - postgres

  backend:
    build:
      context: ./apps/backend
      args:
        GIT_BRANCH: ${GIT_BRANCH:-unknown}
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        BUILD_TIME: ${BUILD_TIME:-unknown}
    env_file:
      - ./secrets/backend.env
    environment:
      UVICORN_WORKERS: "1"
      LOG_LEVEL: "info"
      ENV: "prod"
      DEBUG: "0"
      APP_ENV: "prod"
      ENCRYPTION_ENABLED: "1"
      CRYPTO_STRICT_STARTUP: ${CRYPTO_STRICT_STARTUP:-0}
      GCP_KMS_KEY: "projects/ledgermind-03445-3l/locations/us-east1/keyRings/ledgermind/cryptoKeys/kek"
      GCP_KMS_AAD: "app=ledgermind,env=prod"
      GOOGLE_APPLICATION_CREDENTIALS: /secrets/gcp-sa.json
      # Set CRYPTO_REQUIRED=false to allow startup without functioning KMS (downgrades errors to warnings)
      CRYPTO_REQUIRED: "false"
      # Use DATABASE_URL_FILE secret indirection (populated via entrypoint sh wrapper)
      DATABASE_URL_FILE: /run/secrets/backend_db_url
      CORS_ALLOW_ORIGINS: "https://ledger-mind.org,https://www.ledger-mind.org"
      FRONTEND_ORIGIN: "https://ledger-mind.org"
      COOKIE_SAMESITE: ${COOKIE_SAMESITE:-none}
      COOKIE_SECURE: ${COOKIE_SECURE:-1}
      COOKIE_DOMAIN: ${COOKIE_DOMAIN:-app.ledger-mind.org}
      TRUSTED_PROXY_CIDRS: "172.21.0.0/16"
      OPENAI_BASE_URL: "${OPENAI_BASE_URL:-http://ollama:11434/v1}"
      OPENAI_API_KEY_FILE: /run/secrets/openai_api_key
      OPENAI_FALLBACK_MODEL: "${OPENAI_FALLBACK_MODEL:-gpt-4o-mini}"
      MODEL: "${MODEL:-gpt-oss:20b}"
      DEV_ALLOW_NO_LLM: "${DEV_ALLOW_NO_LLM:-0}"
      ALLOWED_HOSTS: "localhost,127.0.0.1,ledger-mind.org,app.ledger-mind.org,api.ledger-mind.org,nginx"
      ANALYTICS_DB: "1"
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LLM_CONNECT_TIMEOUT: "10"
      LLM_READ_TIMEOUT: "45"
      LLM_INITIAL_RETRY: "1"
      LLM_WARM_WINDOW_S: "60"
      KMS_PROVIDER: "gcp"
      GRPC_VERBOSITY: "ERROR"
      GRPC_TRACE: ""
      HELP_TTL_SECONDS: "86400"
      REPHRASE_VERSION: "v1"
      PRIMARY_MODEL_TAG: "gpt-oss:20b"
      BACKEND_BRANCH: ${BACKEND_BRANCH:-unknown}
      BACKEND_COMMIT: ${BACKEND_COMMIT:-unknown}
      SUGGESTIONS_ENABLED: "1"
    entrypoint: []
    command: >
      python -m uvicorn app.main:app
      --host 0.0.0.0
      --port 8000
      --workers ${UVICORN_WORKERS:-1}
      --proxy-headers
      --forwarded-allow-ips="*"
      --log-level ${LOG_LEVEL:-info}
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    secrets:
      - backend_db_url
    healthcheck:
      test:
        - "CMD"
        - "python"
        - "-c"
        - "import sys,urllib.request; sys.exit(0 if urllib.request.urlopen('http://127.0.0.1:8000/ready').getcode()==200 else 1)"
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    restart: unless-stopped
    volumes:
      # Mount the actual service account JSON file (not the directory) so GOOGLE_APPLICATION_CREDENTIALS points to a file
      - ./secrets/gcp-sa.json/ledgermind-backend-sa.json:/secrets/gcp-sa.json:ro
    networks:
      - shared-ollama

  nginx:
    build:
      context: .
      dockerfile: deploy/Dockerfile.nginx
      args:
        EDGE_GIT_COMMIT: ${GIT_COMMIT:-unknown}
        EDGE_BUILD_TIME: ${BUILD_TIME:-unknown}
        WEB_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        WEB_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        WEB_BUILD_ID: ${WEB_BUILD_ID:-not_set}
        VITE_SUGGESTIONS_ENABLED: ${VITE_SUGGESTIONS_ENABLED:-1}
        VITE_ANALYTICS_ENABLED: ${VITE_ANALYTICS_ENABLED:-0}
        VITE_GIT_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        VITE_GIT_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        GIT_BRANCH: ${VITE_GIT_BRANCH:-unknown}
        GIT_COMMIT: ${VITE_GIT_COMMIT:-unknown}
        BUILD_ID: ${BUILD_ID:-not_set}
    image: ai-finance-agent-oss-clean-nginx:latest
    depends_on:
      backend:
        condition: service_healthy
      agui:
        condition: service_healthy
    volumes:
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt:ro
    ports:
      - "127.0.0.1:80:80"
      - "127.0.0.1:443:443"
    user: "101:101"
    read_only: true
    tmpfs:
      - /var/run:rw,noexec,nosuid,nodev,size=16m,uid=101,gid=101,mode=0755
      - /var/cache/nginx:rw,noexec,nosuid,nodev,size=64m,uid=101,gid=101,mode=0755
      - /tmp:rw,noexec,nosuid,nodev,size=32m,uid=101,gid=101,mode=1777
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    healthcheck:
      test: ["CMD-SHELL","wget -q -O /dev/null http://127.0.0.1:80/_up || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 20s
    restart: unless-stopped
    networks:
      - shared-ollama


  certbot:
    image: certbot/certbot:latest
    volumes:
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt
    entrypoint: ["sh", "-c", "pip install --no-cache-dir certbot-dns-cloudflare && while :; do certbot renew --dns-cloudflare --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini --dns-cloudflare-propagation-seconds 120 --quiet || true; sleep 12h; done"]
    restart: unless-stopped

  nginx-reloader:
    image: docker:cli
    depends_on:
      - nginx
    environment:
      NGINX_CONTAINER: "${NGINX_CONTAINER:-ai-finance-agent-oss-clean-nginx-1}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      sh -c 'while :; do sleep 12h; docker exec "$NGINX_CONTAINER" nginx -s reload || true; done'
    restart: unless-stopped

  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel run
    # Using credentials-file mode via mounted config/credentials. Remove token env to avoid ambiguity.
    # environment:
    #   TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN:-}
    volumes:
      - ./cloudflared:/etc/cloudflared:ro
    ports:
      - "2000:2000" # expose metrics for strict verification
    depends_on:
      nginx:
        condition: service_healthy
    # Note: healthcheck disabled - cloudflared image lacks shell/wget/curl
    # Tunnel status visible via metrics at localhost:2000/metrics (cloudflared_tunnel_ha_connections)
    restart: unless-stopped
    networks:
      - shared-ollama

  cf-health:
    image: curlimages/curl:8.10.1
    command: sh -lc "while :; do curl -sf http://cloudflared:2000/metrics | grep -q cloudflared_tunnel_ha_connections || exit 1; sleep 30; done"
    depends_on:
      cloudflared:
        condition: service_started
    restart: unless-stopped
    networks:
      - shared-ollama

  # Local models runtime (pull & serve models via Ollama API)
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    # Docker Desktop reliable GPU passthrough
    gpus: all
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    volumes:
      # Persist large model downloads across container recreations
      - ollama-models:/root/.ollama
    networks:
      - shared-ollama

  # Sidecar to present 127.0.0.1:11434 inside backend netns, forwarding to ollama service

  agui:
    build:
      context: ./elysia-agui-gateway
    environment:
      BACKEND_BASE: http://backend:8000
      PORT: 3030
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "bun -e 'fetch(`http://127.0.0.1:3030/agui/ping`).then(r=>{if(r.ok)process.exit(0);process.exit(1)}).catch(()=>process.exit(1))'"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 20s
    networks:
      - shared-ollama

networks:
  shared-ollama:
    external: true

volumes:
  pgdata:
  certbot_www:
  letsencrypt:
  # removed named volume 'ollama' in favor of bind mount for model store stability
  ollama-models:

secrets:
  openai_api_key:
    # Use absolute Windows path to avoid Docker Desktop bind issues on Windows
    file: C:/ai-finance-agent-oss-clean/secrets/openai_api_key
  db_password:
    file: ./secrets/db_password.txt
  backend_db_url:
    file: ./secrets/backend_database_url.txt
