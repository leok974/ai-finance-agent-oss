services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: finance
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U myuser -d finance || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  backend:
    build:
      context: ./apps/backend
    environment:
      APP_ENV: "prod"
      ENCRYPTION_ENABLED: ${ENCRYPTION_ENABLED:-1}
      CRYPTO_STRICT_STARTUP: ${CRYPTO_STRICT_STARTUP:-1}
      GCP_KMS_KEY: "projects/ledgermind-03445-3l/locations/global/keyRings/ledgermind/cryptoKeys/backend"
      MASTER_KEK_B64: ${MASTER_KEK_B64}
      ENCRYPTION_MASTER_KEY_BASE64: ${ENCRYPTION_MASTER_KEY_BASE64:-${MASTER_KEK_B64}}
      GCP_KMS_AAD: ${GCP_KMS_AAD:-ledgermind-prod}
      GOOGLE_APPLICATION_CREDENTIALS: /secrets/gcp-sa.json
      DATABASE_URL: "postgresql+psycopg://myuser:${POSTGRES_PASSWORD}@postgres:5432/finance"
      CORS_ALLOW_ORIGINS: "https://ledger-mind.org,https://www.ledger-mind.org"
      FRONTEND_ORIGIN: "https://ledger-mind.org"
      COOKIE_SAMESITE: ${COOKIE_SAMESITE:-none}
      COOKIE_SECURE: ${COOKIE_SECURE:-1}
      COOKIE_DOMAIN: ${COOKIE_DOMAIN:-app.ledger-mind.org}
      TRUSTED_PROXY_CIDRS: "172.21.0.0/16"
      # LLM configuration (local Ollama runtime)
      OPENAI_BASE_URL: ${OPENAI_BASE_URL:-http://ollama:11434/v1}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-ollama}
      MODEL: ${MODEL:-gpt-oss:20b}
      DEV_ALLOW_NO_LLM: ${DEV_ALLOW_NO_LLM:-0}
      ALLOWED_HOSTS: "localhost,127.0.0.1,ledger-mind.org,app.ledger-mind.org,api.ledger-mind.org"
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request,sys; sys.exit(0) if urllib.request.urlopen('http://127.0.0.1:8000/ready').getcode()==200 else sys.exit(1)"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  nginx:
    build:
      context: .
      dockerfile: deploy/Dockerfile.nginx
    image: ai-finance-agent-oss-clean-nginx:latest
    depends_on:
      backend:
        condition: service_healthy
    volumes:
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt:ro
    ports:
      - "127.0.0.1:80:80"
      - "127.0.0.1:443:443"
    restart: unless-stopped

  certbot:
    image: certbot/certbot:latest
    volumes:
      - certbot_www:/var/www/certbot
      - letsencrypt:/etc/letsencrypt
    entrypoint: ["sh", "-c", "pip install --no-cache-dir certbot-dns-cloudflare && while :; do certbot renew --dns-cloudflare --dns-cloudflare-credentials /etc/letsencrypt/cloudflare.ini --dns-cloudflare-propagation-seconds 120 --quiet || true; sleep 12h; done"]
    restart: unless-stopped

  nginx-reloader:
    image: docker:cli
    depends_on:
      - nginx
    environment:
      NGINX_CONTAINER: "${NGINX_CONTAINER:-ai-finance-agent-oss-clean-nginx-1}"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: >
      sh -c 'while :; do sleep 12h; docker exec "$NGINX_CONTAINER" nginx -s reload || true; done'
    restart: unless-stopped

  # Local models runtime (pull & serve models via Ollama API)
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    # Enable NVIDIA GPU acceleration (requires Docker Desktop GPU support enabled)
    gpus: all
    environment:
      - OLLAMA_KEEP_ALIVE=5m
    ports:
      - "11434:11434"

  # Sidecar to present 127.0.0.1:11434 inside backend netns, forwarding to ollama service

  agui:
    build:
      context: ./elysia-agui-gateway
    environment:
      BACKEND_BASE: http://backend:8000
      PORT: 3030
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped

volumes:
  pgdata:
  certbot_www:
  letsencrypt:
  # removed named volume 'ollama' in favor of bind mount for model store stability

